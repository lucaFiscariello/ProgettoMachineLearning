{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import utils\n",
    "\n",
    "import dotenv\n",
    "import pydot\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import multiprocessing.sharedctypes as sharedctypes\n",
    "import os.path\n",
    "import ast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)\n",
    "\n",
    "# Number of samples per 30s audio clip.\n",
    "# TODO: fix dataset to be constant.\n",
    "NB_AUDIO_SAMPLES = 1321967\n",
    "SAMPLING_RATE = 44100\n",
    "\n",
    "# Load the environment from the .env file.\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "\n",
    "def load(filepath):\n",
    "\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    if 'features' in filename:\n",
    "        return pd.read_csv(filepath, index_col=0, header=[0, 1, 2])\n",
    "\n",
    "    if 'echonest' in filename:\n",
    "        return pd.read_csv(filepath, index_col=0, header=[0, 1, 2])\n",
    "\n",
    "    if 'genres' in filename:\n",
    "        return pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    if 'tracks' in filename:\n",
    "        tracks = pd.read_csv(filepath, index_col=0, header=[0, 1])\n",
    "\n",
    "        COLUMNS = [('track', 'tags'), ('album', 'tags'), ('artist', 'tags'),\n",
    "                   ('track', 'genres'), ('track', 'genres_all')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = tracks[column].map(ast.literal_eval)\n",
    "\n",
    "        COLUMNS = [('track', 'date_created'), ('track', 'date_recorded'),\n",
    "                   ('album', 'date_created'), ('album', 'date_released'),\n",
    "                   ('artist', 'date_created'), ('artist', 'active_year_begin'),\n",
    "                   ('artist', 'active_year_end')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = pd.to_datetime(tracks[column])\n",
    "\n",
    "        SUBSETS = ('small', 'medium', 'large')\n",
    "        try:\n",
    "            tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
    "                    'category', categories=SUBSETS, ordered=True)\n",
    "        except (ValueError, TypeError):\n",
    "            # the categories and ordered arguments were removed in pandas 0.25\n",
    "            tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
    "                     pd.CategoricalDtype(categories=SUBSETS, ordered=True))\n",
    "\n",
    "        COLUMNS = [('track', 'genre_top'), ('track', 'license'),\n",
    "                   ('album', 'type'), ('album', 'information'),\n",
    "                   ('artist', 'bio')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = tracks[column].astype('category')\n",
    "\n",
    "        return tracks\n",
    "\n",
    "\n",
    "def get_audio_path(audio_dir, track_id):\n",
    "    \"\"\"\n",
    "    Return the path to the mp3 given the directory where the audio is stored\n",
    "    and the track ID.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import utils\n",
    "    >>> AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    >>> utils.get_audio_path(AUDIO_DIR, 2)\n",
    "    '../data/fma_small/000/000002.mp3'\n",
    "\n",
    "    \"\"\"\n",
    "    tid_str = '{:06d}'.format(track_id)\n",
    "    return os.path.join(audio_dir, tid_str[:3], tid_str + '.mp3')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load metadata and features.\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "genres = utils.load('data/fma_metadata/genres.csv')\n",
    "features = utils.load('data/fma_metadata/features.csv')\n",
    "echonest = utils.load('data/fma_metadata/echonest.csv')\n",
    "\n",
    "small = tracks['set', 'subset'] <= 'small'\n",
    "\n",
    "small = tracks['set', 'subset'] <= 'small'\n",
    "\n",
    "train = tracks['set', 'split'] == 'training'\n",
    "val = tracks['set', 'split'] == 'validation'\n",
    "test = tracks['set', 'split'] == 'test'\n",
    "\n",
    "y_train = tracks.loc[small & train, ('track', 'genre_top')]\n",
    "X_train = features.loc[small & train, 'mfcc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from numpy import array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "AUDIO_DIR = \"data\\\\fma_small\"\n",
    "AUDIO_SAVE = \"figures\\\\out.png\"\n",
    "\n",
    "immagini = []\n",
    "etichette = []\n",
    "\n",
    "all_id_audio_train = y_train.index.values.tolist()\n",
    "i=0\n",
    "\n",
    "for id_audio in all_id_audio_train:\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(1, 1)\n",
    "\n",
    "    filename = utils.get_audio_path(AUDIO_DIR, id_audio)\n",
    "    y, sr = librosa.load(filename, sr=None, mono=True)\n",
    "\n",
    "    librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    D = np.abs(librosa.stft(y))**2\n",
    "    S = librosa.feature.melspectrogram(S=D, sr=sr)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "\n",
    "    img = librosa.display.specshow(S_dB, sr=sr,fmax=8000, ax=ax,cmap='gray')\n",
    "    plt.savefig(AUDIO_SAVE,dpi=80,bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    imm = Image.open(AUDIO_SAVE)\n",
    "    ar = np.array(imm)\n",
    "\n",
    "    immagini.append(ar)\n",
    "    etichette.append(y_train[id_audio])\n",
    "\n",
    "    i=i+1\n",
    "    np.savez(\"training\\\\training_set2.npz\", immagini=immagini, etichette=etichette)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.savez(\"training_set.npz\", immagini=immagini, etichette=etichette)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carica il training set dal file npz\n",
    "training_set = np.load(\"training\\\\training_set.npz\")\n",
    "immagini = training_set[\"immagini\"]\n",
    "etichette = training_set[\"etichette\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(immagini, etichette, test_size=0.2, random_state=42)\n",
    "print(len(etichette))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
