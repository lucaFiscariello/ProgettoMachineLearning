{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generazione dataset CNN\n",
    "Il codice seguente permette la generazione di un dataset a attraverso un'elaborazione degli audio scaricati da FMA. In particolare le operazioni eseguite ,per ogni audio scaricato, per la generazione del dataset sono:\n",
    "- caricamento audio\n",
    "- calcolo dello spettrogramma dell'audio tramite librosa\n",
    "- generazione di un immagine 80x80 pixel che mantiene lo spettrogramma\n",
    "- salvataggio dell'immagine in scala di grigi\n",
    "\n",
    "Ogni riga del dataset generato si compone quindi di 2 elementi: l'immagine in scala di grigi e il target corrispondente che indica il genere dell'audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "import utils\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing.sharedctypes as sharedctypes\n",
    "import os.path\n",
    "import ast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)\n",
    "\n",
    "# Number of samples per 30s audio clip.\n",
    "# TODO: fix dataset to be constant.\n",
    "NB_AUDIO_SAMPLES = 1321967\n",
    "SAMPLING_RATE = 44100\n",
    "\n",
    "# Load the environment from the .env file.\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "\n",
    "def load(filepath):\n",
    "\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    if 'features' in filename:\n",
    "        return pd.read_csv(filepath, index_col=0, header=[0, 1, 2])\n",
    "\n",
    "    if 'echonest' in filename:\n",
    "        return pd.read_csv(filepath, index_col=0, header=[0, 1, 2])\n",
    "\n",
    "    if 'genres' in filename:\n",
    "        return pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    if 'tracks' in filename:\n",
    "        tracks = pd.read_csv(filepath, index_col=0, header=[0, 1])\n",
    "\n",
    "        COLUMNS = [('track', 'tags'), ('album', 'tags'), ('artist', 'tags'),\n",
    "                   ('track', 'genres'), ('track', 'genres_all')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = tracks[column].map(ast.literal_eval)\n",
    "\n",
    "        COLUMNS = [('track', 'date_created'), ('track', 'date_recorded'),\n",
    "                   ('album', 'date_created'), ('album', 'date_released'),\n",
    "                   ('artist', 'date_created'), ('artist', 'active_year_begin'),\n",
    "                   ('artist', 'active_year_end')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = pd.to_datetime(tracks[column])\n",
    "\n",
    "        SUBSETS = ('small', 'medium', 'large')\n",
    "        try:\n",
    "            tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
    "                    'category', categories=SUBSETS, ordered=True)\n",
    "        except (ValueError, TypeError):\n",
    "            # the categories and ordered arguments were removed in pandas 0.25\n",
    "            tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
    "                     pd.CategoricalDtype(categories=SUBSETS, ordered=True))\n",
    "\n",
    "        COLUMNS = [('track', 'genre_top'), ('track', 'license'),\n",
    "                   ('album', 'type'), ('album', 'information'),\n",
    "                   ('artist', 'bio')]\n",
    "        for column in COLUMNS:\n",
    "            tracks[column] = tracks[column].astype('category')\n",
    "\n",
    "        return tracks\n",
    "\n",
    "\n",
    "def get_audio_path(audio_dir, track_id):\n",
    "    \"\"\"\n",
    "    Return the path to the mp3 given the directory where the audio is stored\n",
    "    and the track ID.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import utils\n",
    "    >>> AUDIO_DIR = os.environ.get('AUDIO_DIR')\n",
    "    >>> utils.get_audio_path(AUDIO_DIR, 2)\n",
    "    '../data/fma_small/000/000002.mp3'\n",
    "\n",
    "    \"\"\"\n",
    "    tid_str = '{:06d}'.format(track_id)\n",
    "    return os.path.join(audio_dir, tid_str[:3], tid_str + '.mp3')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load metadata and features.\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "genres = utils.load('data/fma_metadata/genres.csv')\n",
    "features = utils.load('data/fma_metadata/features.csv')\n",
    "echonest = utils.load('data/fma_metadata/echonest.csv')\n",
    "\n",
    "small = tracks['set', 'subset'] <= 'small'\n",
    "y_train = tracks.loc[small, ('track', 'genre_top')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from numpy import array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "AUDIO_DIR = \"data\\\\fma_small\"\n",
    "AUDIO_SAVE = \"out.png\"\n",
    "\n",
    "immagini = []\n",
    "etichette = []\n",
    "\n",
    "all_id_audio_train = y_train.index.values.tolist()\n",
    "i=0\n",
    "\n",
    "for id_audio in all_id_audio_train[0:1]:\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(1, 1)\n",
    "\n",
    "    filename = utils.get_audio_path(AUDIO_DIR, id_audio)\n",
    "    y, sr = librosa.load(filename, sr=None, mono=True)\n",
    "\n",
    "    librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    D = np.abs(librosa.stft(y))**2\n",
    "    S = librosa.feature.melspectrogram(S=D, sr=sr)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "\n",
    "    img = librosa.display.specshow(S_dB, sr=sr,fmax=8000, ax=ax,cmap='gray')\n",
    "    plt.savefig(AUDIO_SAVE,dpi=80,bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    imm = Image.open(AUDIO_SAVE)\n",
    "    ar = np.array(imm)\n",
    "\n",
    "    immagini.append(ar)\n",
    "    etichette.append(y_train[id_audio])\n",
    "\n",
    "    i=i+1\n",
    "    np.savez(\"Cnn_data\\\\Cnn_data.npz\", immagini=immagini, etichette=etichette)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Carica il training set dal file npz\n",
    "training_set = np.load(\"Cnn_data\\\\Cnn_data.npz\")\n",
    "immagini = training_set[\"immagini\"]\n",
    "etichette = training_set[\"etichette\"]\n",
    "\n",
    "print(len(etichette))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
